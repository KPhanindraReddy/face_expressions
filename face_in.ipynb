{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNzr0HCQepohhHp3WlHeBdp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KPhanindraReddy/face_expressions/blob/main/face_in.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install openvino-dev[onnx]\n",
        "!pip install mediapipe\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "from openvino.runtime import Core\n",
        "from IPython.display import display, HTML\n",
        "from base64 import b64decode, b64encode\n",
        "import os\n",
        "from google.colab.output import eval_js  # Import the missing function\n",
        "\n",
        "# Download OpenVINO models\n",
        "if not os.path.exists('model'):\n",
        "    os.makedirs('model')\n",
        "\n",
        "!omz_downloader --name face-detection-adas-0001 --precisions FP32 -o model\n",
        "!omz_downloader --name facial-landmarks-35-adas-0002 --precisions FP32 -o model\n",
        "!omz_downloader --name emotions-recognition-retail-0003 --precisions FP32 -o model\n",
        "\n",
        "# Initialize OpenVINO core\n",
        "ie = Core()\n",
        "\n",
        "# Load models\n",
        "print(\"Loading OpenVINO models...\")\n",
        "face_model = ie.read_model(model='model/intel/face-detection-adas-0001/FP32/face-detection-adas-0001.xml')\n",
        "face_compiled_model = ie.compile_model(model=face_model, device_name=\"CPU\")\n",
        "face_input_layer = face_compiled_model.input(0)\n",
        "_, _, H, W = face_input_layer.shape\n",
        "print(f\"Face detection input shape: {H}x{W}\")\n",
        "\n",
        "landmark_model = ie.read_model(model='model/intel/facial-landmarks-35-adas-0002/FP32/facial-landmarks-35-adas-0002.xml')\n",
        "landmark_compiled_model = ie.compile_model(model=landmark_model, device_name=\"CPU\")\n",
        "landmark_input_layer = landmark_compiled_model.input(0)\n",
        "_, _, LANDMARK_H, LANDMARK_W = landmark_input_layer.shape\n",
        "print(f\"Landmark model input shape: {LANDMARK_H}x{LANDMARK_W}\")\n",
        "\n",
        "emotion_model = ie.read_model(model='model/intel/emotions-recognition-retail-0003/FP32/emotions-recognition-retail-0003.xml')\n",
        "emotion_compiled_model = ie.compile_model(model=emotion_model, device_name=\"CPU\")\n",
        "emotion_input_layer = emotion_compiled_model.input(0)\n",
        "_, _, EMOTION_H, EMOTION_W = emotion_input_layer.shape\n",
        "print(f\"Emotion model input shape: {EMOTION_H}x{EMOTION_W}\")\n",
        "\n",
        "emotion_labels = ['neutral', 'happy', 'sad', 'surprise', 'anger']\n",
        "\n",
        "print(\"Models loaded successfully!\")\n",
        "\n",
        "# Setup webcam with JavaScript\n",
        "display(HTML('''\n",
        "<div style=\"display: flex; flex-direction: column; align-items: center;\">\n",
        "    <div>\n",
        "        <video id=\"video\" width=\"640\" height=\"480\" autoplay style=\"display:none;\"></video>\n",
        "        <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n",
        "        <img id=\"outputImage\" width=\"640\" height=\"480\" style=\"border: 2px solid #4CAF50; border-radius: 5px;\">\n",
        "    </div>\n",
        "    <div style=\"margin-top: 20px;\">\n",
        "        <button id=\"stopButton\" style=\"padding:10px;font-size:16px;margin:10px;background:#f44336;color:white;border:none;border-radius:5px;\">\n",
        "            Stop Monitoring\n",
        "        </button>\n",
        "        <div id=\"status\" style=\"padding:10px;font-family:Arial;font-size:16px;\">Initializing camera...</div>\n",
        "    </div>\n",
        "</div>\n",
        "<script>\n",
        "// Create monitoring state on window object\n",
        "window.monitoringActive = true;\n",
        "const video = document.getElementById('video');\n",
        "const canvas = document.getElementById('canvas');\n",
        "const outputImage = document.getElementById('outputImage');\n",
        "const stopButton = document.getElementById('stopButton');\n",
        "const statusDiv = document.getElementById('status');\n",
        "\n",
        "// Function to capture frame\n",
        "function captureFrame() {\n",
        "    if (video.videoWidth === 0 || video.videoHeight === 0) {\n",
        "        return '';\n",
        "    }\n",
        "    canvas.width = video.videoWidth;\n",
        "    canvas.height = video.videoHeight;\n",
        "    const ctx = canvas.getContext('2d');\n",
        "    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);\n",
        "    return canvas.toDataURL('image/jpeg', 0.8);\n",
        "}\n",
        "\n",
        "// Function to update output image\n",
        "function updateOutputImage(dataUrl) {\n",
        "    outputImage.src = dataUrl;\n",
        "}\n",
        "\n",
        "// Function to check monitoring state\n",
        "function isMonitoringActive() {\n",
        "    return window.monitoringActive;\n",
        "}\n",
        "\n",
        "// Camera setup\n",
        "async function setupCamera() {\n",
        "    try {\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({\n",
        "            video: { width: 640, height: 480, facingMode: 'user' }\n",
        "        });\n",
        "        video.srcObject = stream;\n",
        "        statusDiv.textContent = 'Camera ready. Monitoring active...';\n",
        "        return true;\n",
        "    } catch (err) {\n",
        "        statusDiv.textContent = 'Camera error: ' + err.message;\n",
        "        return false;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Stop button handler\n",
        "stopButton.addEventListener('click', function() {\n",
        "    window.monitoringActive = false;\n",
        "    if (video.srcObject) {\n",
        "        video.srcObject.getTracks().forEach(track => track.stop());\n",
        "    }\n",
        "    statusDiv.textContent = 'Monitoring stopped.';\n",
        "    stopButton.textContent = 'Stopped';\n",
        "    stopButton.disabled = true;\n",
        "    stopButton.style.background = '#9e9e9e';\n",
        "});\n",
        "\n",
        "// Initialize camera\n",
        "setupCamera();\n",
        "</script>\n",
        "'''))\n",
        "\n",
        "# Define state colors\n",
        "state_colors = {\n",
        "    'engaged': (0, 255, 0),          # Green - Engaged\n",
        "    'disengaged': (0, 0, 255),        # Red - Disengaged\n",
        "    'looking_down': (255, 255, 0),    # Yellow - Looking down\n",
        "    'unknown': (128, 128, 128)        # Gray - Unknown\n",
        "}\n",
        "\n",
        "print(\"Starting student engagement monitoring with OpenVINO...\")\n",
        "print(\"Click 'Stop' button to exit\")\n",
        "\n",
        "def get_frame():\n",
        "    \"\"\"Capture frame using predefined JavaScript function\"\"\"\n",
        "    return eval_js(\"captureFrame()\")\n",
        "\n",
        "def update_output(data_url):\n",
        "    \"\"\"Update output image in browser\"\"\"\n",
        "    display(HTML(f\"<script>updateOutputImage('{data_url}');</script>\"))\n",
        "\n",
        "# ----------------------------\n",
        "# Face Detection with OpenVINO\n",
        "# ----------------------------\n",
        "def detect_faces_ov(frame):\n",
        "    # Preprocess input image\n",
        "    resized = cv2.resize(frame, (W, H))\n",
        "    input_image = np.expand_dims(resized.transpose(2, 0, 1), axis=0)\n",
        "\n",
        "    # Inference\n",
        "    results = face_compiled_model([input_image])[face_compiled_model.output(0)]\n",
        "    faces = []\n",
        "\n",
        "    # Process results\n",
        "    for detection in results[0][0]:\n",
        "        confidence = detection[2]\n",
        "        if confidence > 0.3:  # Lower confidence threshold\n",
        "            x_min = int(detection[3] * frame.shape[1])\n",
        "            y_min = int(detection[4] * frame.shape[0])\n",
        "            x_max = int(detection[5] * frame.shape[1])\n",
        "            y_max = int(detection[6] * frame.shape[0])\n",
        "            w = x_max - x_min\n",
        "            h = y_max - y_min\n",
        "            # Filter out small detections\n",
        "            if w > 30 and h > 30:  # More lenient size filter\n",
        "                faces.append((x_min, y_min, w, h))\n",
        "\n",
        "    return faces\n",
        "\n",
        "# ----------------------------\n",
        "# Facial Landmarks with OpenVINO\n",
        "# ----------------------------\n",
        "def detect_landmarks_ov(face_roi):\n",
        "    # Preprocess input image with CORRECT dimensions\n",
        "    resized = cv2.resize(face_roi, (LANDMARK_W, LANDMARK_H))\n",
        "    input_image = np.expand_dims(resized.transpose(2, 0, 1), axis=0).astype(np.float32)\n",
        "\n",
        "    # Inference\n",
        "    landmarks = landmark_compiled_model([input_image])[landmark_compiled_model.output(0)]\n",
        "    return landmarks[0].reshape(-1, 2)\n",
        "\n",
        "# ----------------------------\n",
        "# Emotion Recognition with OpenVINO\n",
        "# ----------------------------\n",
        "def detect_emotion_ov(face_roi):\n",
        "    # Preprocess input image with CORRECT dimensions\n",
        "    resized = cv2.resize(face_roi, (EMOTION_W, EMOTION_H))\n",
        "    input_image = np.expand_dims(resized.transpose(2, 0, 1), axis=0).astype(np.float32)\n",
        "\n",
        "    # Inference\n",
        "    emotions = emotion_compiled_model([input_image])[emotion_compiled_model.output(0)]\n",
        "    emotion_idx = np.argmax(emotions)\n",
        "    confidence = emotions[0][emotion_idx]\n",
        "\n",
        "    return emotion_labels[emotion_idx], confidence\n",
        "\n",
        "# ----------------------------\n",
        "# Attention State Detection (Simplified)\n",
        "# ----------------------------\n",
        "def determine_state(face_position, frame_width):\n",
        "    x, y, w, h = face_position\n",
        "\n",
        "    # Calculate face center\n",
        "    face_center_x = x + w//2\n",
        "    frame_center_x = frame_width // 2\n",
        "    position_ratio = (face_center_x - frame_center_x) / frame_center_x\n",
        "\n",
        "    # Simple attention detection\n",
        "    if abs(position_ratio) > 0.3:\n",
        "        return 'disengaged'\n",
        "\n",
        "    return 'engaged'\n",
        "\n",
        "# Initialize tracking\n",
        "frame_count = 0\n",
        "start_time = time.time()\n",
        "student_counts = []\n",
        "emotion_counts = {e: 0 for e in emotion_labels}\n",
        "state_counts = {s: 0 for s in state_colors}\n",
        "\n",
        "try:\n",
        "    print(\"Initializing camera...\")\n",
        "    time.sleep(3)  # Allow camera initialization\n",
        "\n",
        "    while frame_count < 300:  # Process up to 300 frames\n",
        "        frame_count += 1\n",
        "\n",
        "        # Check if monitoring should stop\n",
        "        if not eval_js('isMonitoringActive()'):\n",
        "            print(\"Monitoring stopped by user\")\n",
        "            break\n",
        "\n",
        "        # Get frame from JS\n",
        "        js_reply = get_frame()\n",
        "        if not js_reply or ',' not in js_reply:\n",
        "            continue\n",
        "\n",
        "        # Decode image\n",
        "        _, data = js_reply.split(',', 1)\n",
        "        nparr = np.frombuffer(b64decode(data), np.uint8)\n",
        "        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "        if frame is None or frame.size == 0:\n",
        "            continue\n",
        "\n",
        "        # Mirror and process frame\n",
        "        frame = cv2.flip(frame, 1)\n",
        "        annotated_frame = frame.copy()\n",
        "        height, width = frame.shape[:2]\n",
        "\n",
        "        # Detect faces with OpenVINO\n",
        "        faces = detect_faces_ov(frame)\n",
        "        student_count = len(faces)\n",
        "        student_counts.append(student_count)\n",
        "\n",
        "        # Process each face\n",
        "        for (x, y, w, h) in faces:\n",
        "            # Get face ROI\n",
        "            face_roi = frame[y:y+h, x:x+w]\n",
        "            if face_roi.size == 0:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Detect emotion\n",
        "                emotion, confidence = detect_emotion_ov(face_roi)\n",
        "                emotion_counts[emotion] += 1\n",
        "\n",
        "                # Determine attention state\n",
        "                state = determine_state((x, y, w, h), width)\n",
        "                state_counts[state] = state_counts.get(state, 0) + 1\n",
        "\n",
        "                # Draw face bounding box\n",
        "                color = state_colors.get(state, (128, 128, 128))\n",
        "                cv2.rectangle(annotated_frame, (x, y), (x+w, y+h), color, 3)  # Thicker border\n",
        "\n",
        "                # Add labels\n",
        "                label = f\"{state.capitalize()} | {emotion.capitalize()}\"\n",
        "                text_y = y - 10 if y > 30 else y + 30\n",
        "                cv2.putText(annotated_frame, label, (x, text_y),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)  # Larger font\n",
        "\n",
        "                # Add confidence indicator\n",
        "                cv2.putText(annotated_frame, f\"{confidence*100:.1f}%\",\n",
        "                           (x, text_y + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 0), 2)\n",
        "\n",
        "            except Exception as e:\n",
        "                # Skip this face but continue processing\n",
        "                continue\n",
        "\n",
        "        # Add metrics\n",
        "        elapsed_time = time.time() - start_time\n",
        "        fps = frame_count / elapsed_time if elapsed_time > 0 else 0\n",
        "        cv2.putText(annotated_frame, f\"FPS: {fps:.1f}\", (10, 30),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "        cv2.putText(annotated_frame, f\"Students: {student_count}\", (10, 70),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "        cv2.putText(annotated_frame, f\"Frame: {frame_count}/300\", (10, 110),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "        # Add status indicator\n",
        "        status_text = \"Monitoring Active\" if eval_js('isMonitoringActive()') else \"Monitoring Stopped\"\n",
        "        status_color = (0, 255, 0) if eval_js('isMonitoringActive()') else (0, 0, 255)\n",
        "        cv2.putText(annotated_frame, status_text, (width - 250, 30),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, status_color, 2)\n",
        "\n",
        "        # Display results\n",
        "        _, buffer = cv2.imencode('.jpg', annotated_frame)\n",
        "        data_url = f\"data:image/jpeg;base64,{b64encode(buffer).decode()}\"\n",
        "        update_output(data_url)\n",
        "\n",
        "        # Print periodic summary\n",
        "        if frame_count % 10 == 0:  # Print every 10 frames\n",
        "            print(f\"Frame {frame_count}: Faces: {student_count}\")\n",
        "\n",
        "        time.sleep(0.01)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    import traceback\n",
        "    print(traceback.format_exc())\n",
        "\n",
        "# Final report\n",
        "if frame_count > 0:\n",
        "    avg_students = np.mean(student_counts) if student_counts else 0\n",
        "    elapsed_time = time.time() - start_time\n",
        "    avg_fps = frame_count / elapsed_time if elapsed_time > 0 else 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CLASSROOM MONITORING REPORT\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Monitoring duration: {elapsed_time:.1f} seconds\")\n",
        "    print(f\"Total frames processed: {frame_count}\")\n",
        "    print(f\"Average FPS: {avg_fps:.1f}\")\n",
        "    print(f\"Average students detected: {avg_students:.1f}\")\n",
        "\n",
        "    print(\"\\nATTENTION STATES DISTRIBUTION:\")\n",
        "    total_states = sum(state_counts.values())\n",
        "    for state, count in state_counts.items():\n",
        "        percentage = (count / total_states) * 100 if total_states > 0 else 0\n",
        "        print(f\"- {state.upper()}: {count} frames ({percentage:.1f}%)\")\n",
        "\n",
        "    print(\"\\nEMOTION DISTRIBUTION:\")\n",
        "    total_emotions = sum(emotion_counts.values())\n",
        "    for emotion, count in emotion_counts.items():\n",
        "        percentage = (count / total_emotions) * 100 if total_emotions > 0 else 0\n",
        "        print(f\"- {emotion.upper()}: {count} frames ({percentage:.1f}%)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"MONITORING COMPLETE\")\n",
        "    print(\"=\"*50)\n",
        "else:\n",
        "    print(\"No frames processed. Please check camera access and try again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        },
        "id": "9rYFvBO9j7-W",
        "outputId": "799777e8-97bf-43fe-a020-172a00e437bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################|| Downloading facial-landmarks-35-adas-0002 ||################\n",
            "\n",
            "========== Downloading model/intel/facial-landmarks-35-adas-0002/FP32/facial-landmarks-35-adas-0002.xml\n",
            "... 100%, 248 KB, 1076 KB/s, 0 seconds passed\n",
            "\n",
            "========== Downloading model/intel/facial-landmarks-35-adas-0002/FP32/facial-landmarks-35-adas-0002.bin\n",
            "... 100%, 17950 KB, 21081 KB/s, 0 seconds passed\n",
            "\n",
            "object address  : 0x78947020c940\n",
            "object refcount : 2\n",
            "object type     : 0x9d7580\n",
            "object type name: KeyboardInterrupt\n",
            "object repr     : KeyboardInterrupt()\n",
            "lost sys.stderr\n",
            "^C\n",
            "################|| Downloading emotions-recognition-retail-0003 ||################\n",
            "\n",
            "========== Downloading model/intel/emotions-recognition-retail-0003/FP32/emotions-recognition-retail-0003.xml\n",
            "... 100%, 38 KB, 500 KB/s, 0 seconds passed\n",
            "\n",
            "========== Downloading model/intel/emotions-recognition-retail-0003/FP32/emotions-recognition-retail-0003.bin\n",
            "\n",
            "^C\n",
            "Loading OpenVINO models...\n",
            "Face detection input shape: 384x672\n",
            "Landmark model input shape: 60x60\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Exception from src/inference/src/cpp/core.cpp:90:\nException from src/frontends/ir/src/ir_deserializer.cpp:355:\nIncorrect weights in bin file!\n\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-2542265826.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Landmark model input shape: {LANDMARK_H}x{LANDMARK_W}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0memotion_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model/intel/emotions-recognition-retail-0003/FP32/emotions-recognition-retail-0003.xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0memotion_compiled_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memotion_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CPU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0memotion_input_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memotion_compiled_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openvino/runtime/ie_api.py\u001b[0m in \u001b[0;36mread_model\u001b[0;34m(self, model, weights)\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     def compile_model(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Exception from src/inference/src/cpp/core.cpp:90:\nException from src/frontends/ir/src/ir_deserializer.cpp:355:\nIncorrect weights in bin file!\n\n"
          ]
        }
      ]
    }
  ]
}